# <p align="center"> SimBEV: A Synthetic Multi-Task Multi-Sensor Driving Data Generation Tool and Dataset </p>

<p align="center">
  <a href="https://arxiv.org/abs/2502.01894" alt="SimBEV Paper">
      <img src="https://img.shields.io/badge/arXiv%20Paper-lavender?logo=arxiv&color=%23B31B1B" /></a>
  <a href="https://drive.google.com/drive/folders/14MytQeGmW80Btg_AGPNrE18ZLdLzyGx5" alt="SimBEV Dataset">
      <img src="https://img.shields.io/badge/Dataset-blue?logo=google%20drive&logoColor=%23FFFFFF&logoSize=auto" /></a>
  <a href="https://simbev.org" alt="SimBEV Project Page">
      <img src="https://img.shields.io/badge/Project%20Page-green?color=%2300CC00&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J/AAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfpBBUSAyKON4S+AAAECUlEQVRIx4WVXUzVdRjHP//D4XhI3BE3yRd0QKArC2Li1gWzsFqt1nq5cV540VYXtNVN3XijW6vFVr5cyEZzaktzaBf4Mm2Ks7ms+dIbpJtiEnAAQ1OgAzQIzqcL/pxEoJ6L//bs/3u+v+f/PP/v9xswLYQsllJGGUXkAX38RgstdDMe8J8hYtxq6201Zdq04446ZtqUrdZbbVz8j/LAShtMmXSPjaY97Pd+6y7TfuNnJk3ZYKWBs5THrbHHbjdbZKW/etaHPOkpF3nIW75okZvttsca485Qnmutwx61XMzzK9utNGqTTWZb5A/+bJFY7lGHrTXXabfXOuwO80R81yHfEGM22WRMfN677jQq5rnDYWvv6ULEGkc9YkLEFbbZ6NwpAFlus89qERMecdSacJwirrHDYW/7sSsM/NA+n5QpAFjqDQ8Zt9RPvO2QHVaKYfsNdvqKdQ7YYb2dfumcaQD4kQN+YbsD1vmynTYYnwCodsDNYrZr3We/es2tbrTKcs95znKr3OhWm9VB97nWbHGLA1YLRqy309LwlhyP+7unTDpoyjuOOOIdUw6a9ISXveyD4clSO603EqWAas5wAwKEPFZylLdZRCHLWcKbwC566KSdbmr4gEJ6A4QbnKGagihlLOYM6XAlheRziRE6gg4hxjPAdkaDCY78RIRHuSBAmjO8RlmEcsa4AsEkQITrmez+SNJPKYTvrzBGeZRiUvSRbQCMsYRRBogaASBGAMSY3Phf9LM0zKWPFMVREuSzlxEmuixmLjsZClsIqAAOZwiYxXLyOR7+f3PIJxGZRor/ofx0Fu62w2KzjRkz4nvetcKoMWPGzPW0p80Ns2zz/cX9mazYDndHaWMeebSFc+4mxnzGwmziMZrZQpz5dDOaWfk82iI0E2UVk5/ZTpoVzKY6BSS4PonLKqI0R2jhJuuYnEU7vawhIDDGApaRQw7LWGDMAKjAzMojrOMmLVG6+JoXKKEVgFtcYi2v8wiPUUCChcBZBujiMhd4ibbwHJSwjhN0RUhzkAQbQHiAp1hMKXWsJ85FDtBFFwe4SJz17OU5slhNjgAbSHCQ9CSdk1ZaZaODdtjnBVeaY5Chc2COhR50xG5TNlplpcl/6YyVdnrVXtvcZJHvZ7TnXj0o8qqNPuwm2+z1qp2hoGQkbcRWK0Qs8brHnDcFIGKt/T4rYoWtjmQk7R5RHXJbKKrvOOxbUwCe9g8/NVvMc7tDU0R1BllPeMykT2RkfbnnvWLJrLJ+n7FssdjHveZ3rvSkp1zofu/4qsVumdVYQojA1TaYsss9HnLc4/7oOesc97yfh9a2ehZry4xz0lz/DM11xL/VlNdmNtdgJhiyWEI5ZRQxnyC092Z6ZrL3fwBmmHFVhXdUNwAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyNS0wNC0yMVQxODowMTo1NSswMDowMOY/5WAAAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjUtMDQtMjFUMTg6MDE6MzQrMDA6MDD3el/vAAAAAElFTkSuQmCC" /></a>
  <a href="https://youtu.be/5hDH31RDk8c" alt="SimBEV Video">
      <img src="https://img.shields.io/badge/Video-green?logo=youtube&logoColor=%23FFFFFF&color=%23FF0000" /></a>
</p>
<p align="center">
  <a href="https://scholar.google.com/citations?hl=en&user=zXkVUDgAAAAJ">Goodarz Mehr</a>, <a href="https://scholar.google.com/citations?user=mLc5_pcAAAAJ">Azim Eskandarian</a>
<br>
Virginia Commonwealth University
</p>

https://github.com/user-attachments/assets/af074eff-b30c-43e0-b544-1b38b77d3345

## Table of Contents

- [News](#news)
- [About](#about)
- [Installation](#installation)
  - [Hardware Requirements](#hardware-requirements)
  - [CARLA](#carla)
  - [SimBEV](#simbev)
- [Usage](#usage)
  - [Creating/Expanding/Replacing a SimBEV Dataset](#creatingexpandingreplacing-a-simbev-dataset)
  - [Post-processing](#post-processing)
  - [Data Visualization](#data-visualization)
  - [Using the SimBEV Dataset](#using-the-simbev-dataset)
- [Data Format](#data-format)
  - [Sensor Setup](#sensor-setup)
  - [Folder Structure](#folder-structure)
    - [configs](#configs)
    - [ground-truth](#ground-truth)
    - [infos](#infos)
    - [logs](#logs)
    - [sweeps](#sweeps)
- [SimBEV Dataset Benchmarks](#simbev-dataset-benchmarks)
  - [3D Object Detection](#3d-object-detection)
  - [BEV Segmentation](#bev-segmentation)
- [Acknowledgement](#acknowledgement)
- [Citation](#citation)

## News
**[2025/4/15]** We released [our implementation](https://github.com/GoodarzMehr/UniTR) of [UniTR](https://github.com/Haiyang-W/UniTR) trained on the SimBEV dataset.

**[2025/2/9]** We released [our implementation](https://github.com/GoodarzMehr/bevfusion) of [BEVFusion](https://github.com/mit-han-lab/bevfusion) trained on the SimBEV dataset.

**[2025/2/6]** Initial release of dataset, code, and paper.

## About

SimBEV is a configurable and scalable synthetic driving data generation tool based on the CARLA Simulator that incorporates information from various sources to capture accurate bird's-eye view (BEV) ground truth data. SimBEV supports a comprehensive array of sensors and enables a variety of perception tasks, including BEV segmentation and 3D object detection. We used SimBEV to create the SimBEV dataset, a large collection of annotated perception data from diverse driving scenarios.

![collage](assets/Collage.png)

###### <p align="center"> A data sample generated by SimBEV. The left half depicts a 360-degree view of the ego (data collection) vehicle's surroundings produced by different camera types (from top to bottom RGB, sematic segmentation, instance segmentation, depth, and optical flow cameras, respectively). On the right half, views of lidar, semantic lidar, radar, and BEV ground truth data are shown from top to bottom, respectively. Some images also contain 3D object bounding boxes. </p>

SimBEV randomizes as many simulation parameters as possible to create a diverse set of scenarios. To create a new dataset, SimBEV generates and collects data from consecutive episodes, or scenes. The user configures the desired number of scenes for each map (can be an existing CARLA map or a custom one) for the training, validation, and test sets. The user can configure a variety of simulation parameters and choose which sensors to use. The user can add more scenes to an existing SimBEV dataset or replace individual scenes.

SimBEV currently supports five camera types (RGB, semantic segmentation, instance segmentation, depth, and optical flow), lidar, semantic lidar, radar, GNSS, and IMU. The user has full control over each sensor's characteristics (e.g. camera resolution or number of lidar channels), but placement of the sensors is fixed for now. SimBEV currently offers two annotation types: 3D object bounding boxes and BEV ground truth.

The SimBEV dataset is a collection of 320 scenes spread across 11 CARLA maps and contains data from all supported sensors. With each scene lasting 16 seconds at a frame rate of 20 Hz, the SimBEV dataset contains 102,400 annotated frames, 8,315,935 3D object bounding boxes (3,792,499 of which are _valid_ (i.e. have at least one lidar or radar point inside)), and 2,793,491,357 BEV ground truth labels.

## Installation

### Hardware Requirements

We developed and tested SimBEV on a system with the following specifications:
* AMD Ryzen 9 3900X (Any Intel 9th gen or newer or Ryzen 7/9 CPU will probably work)
* 64 GB RAM (32 GB is probably enough)
* Nvidia GeForce RTX 3090
* Ubuntu 22.04

To run SimBEV, your system must satisfy CARLA 0.9.15's [minimum system requirements](https://github.com/carla-simulator/carla/tree/ue4-dev?tab=readme-ov-file#recommended-system). You must also have an Nvidia graphics card with at least 16 (preferrably 24) GB of VRAM (RTX 3090/3090 Ti/4080/4080 Super/4090/5070 Ti/5080/5090 or one of the pro models).

### CARLA

SimBEV works best with our modified version of CARLA 0.9.15 that has an enhanced content library. Some of the enhancements are:
* We added three new sports cars to CARLA's vehicle library using existing 3D models: sixth generation Ford Mustang, Toyota GR Supra, and Bugatti Chiron. The Ford Mustang is SimBEV's default vehicle for collecting data.

https://github.com/user-attachments/assets/71fa7403-7e5f-4365-b385-5f5fd3801049

* We added lights (headlights, taillights, blinkers, etc.) to those older vehicle models in CARLA's library that lacked them, and redesigned existing vehicle lights in Blender using a new multi-layer approach that better visualizes modern multi-purpose lights.

![mustang-lights](assets/MustangLights.png)

* We added a set of 160 standard paint colors for most vehicle models (apart from a few like the firetruck) to choose from, and fixed paint color randomization issues for a few vehicles (e.g. the bus).
* We updated the vehicle dynamics parameters of vehicle models to better match their vehicle's behavior and performance in the real world.
* We added or updated pedestrian navigation information for CARLA's Town12, Town13, and Town15 maps.
* We updated motorcycle and bicycle models to select their driver model randomly, instead of always sticking with the same model.
* We added lights to buildings in Town12 and fixed issues that prevented full control over building/street lights in Town12 and Town15.

SimBEV is compatible with the standard version of CARLA 0.9.15, but some features may not work properly.

**We have not tested SimBEV with CARLA 0.10.0 and advise against using it with SimBEV. While CARLA 0.10.0 offers superior graphics, it lacks some features from CARLA 0.9.15 that SimBEV relies on (e.g. customizable weather, large maps, etc.) We will make SimBEV available for CARLA 0.10.\* when it reaches feature parity with CARLA 0.9.15.**

1. Download [CARLA 0.9.15 for Ubuntu](https://github.com/carla-simulator/carla/releases/tag/0.9.15) to your desired directory and unzip it.
2. Download our [enhanced content library](https://drive.google.com/file/d/16y3nM27-c1z-_up1T3Qreyw6yr49Q3Qt/view) and unzip it. In your CARLA directory, under `CarlaUE4`, rename the `Content` folder to `Content.bak` to have a backup of the original CARLA content library. Copy the downloaded `Content` folder to the `CarlaUE4` directory.
3. Open `CarlaUE4/Config/DefaultEngine.ini` and replace all mentions of `Town10HD_Opt` with `Town10HD`.

### SimBEV

We recommend using SimBEV with Docker. The base Docker image is Ubuntu 20.04 with CUDA 11.3 and Vulkan SDK 1.3.204.1. If you want to use a different base image, you may have to modify `ubuntu2004/x86_64` when fetching keys on line 62 of the [Dockerfile](Dockerfile), based on your Ubuntu release and system architecture. **Ensure that `libnvidia-gl` and `libnvidia-common` version numbers on line 66 of the [Dockerfile](Dockerfile) match your Nvidia driver version number.**

1. Install [Docker](https://docs.docker.com/engine/install/) on your system.
2. Install the [Nvidia Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installation-guide).
It exposes your Nvidia graphics card to Docker containers.
3. Clone this repository:
   ```Bash
   git clone https://github.com/GoodarzMehr/SimBEV.git && cd SimBEV
   ```
4. Build the SimBEV Docker image (this will take several minutes):
   ```Bash
   docker build --no-cache --rm --build-arg ARG -t simbev:develop .
   ```
   The following optional build arguments (`ARG`) are available:
   * `USER`: username inside each container, set to _sb_ by default.
   * `CARLA_VERSION`: installed CARLA version, set to _0.9.15_ by default.
5. Launch a container:
   ```Bash
   docker run --privileged --gpus all --network=host -e DISPLAY=$DISPLAY \
   -v [path/to/CARLA]:/home/carla \
   -v [path/to/SimBEV]:/home/simbev \
   -v [path/to/dataset]:/dataset \
   --shm-size 32g -it simbev:develop /bin/bash
   ```
   Use `nvidia-smi` to ensure your graphics card is visible inside the container.

If you would like to use SimBEV without Docker, you can replicate the steps in the [Dockerfile](Dockerfile) to install the dependencies. Be sure to add the CARLA `.egg` file (located in `[path/to/CARLA]/PythonAPI/carla/dist/`) to the `PYTHONPATH` environment variable.

## Usage

### Creating/Expanding/Replacing a SimBEV Dataset

In the [simbev](simbev) directory, use the [config.yaml](simbev/config.yaml) file to configure SimBEV's behavior (for a detailed explanation of available parameters see the [sample_config.yaml](simbev/sample_config.yaml) file). Then run
```Bash
python simbev.py config.yaml [options]
```
`options` can be any of the following:
* `path`: path for saving the dataset (`/dataset` by default).
* `render`: visualize captured sensor data.
* `save`: save captured sensor data (used by default).
* `no-save`: do not save captured sensor data.

For instance,
```Bash
python simbev.py config.yaml --render --no-save
```
visualizes sensor data as it is captured without saving it.

Set `mode` in the [config.yaml](simbev/config.yaml) file to `create` to create a new SimBEV dataset. If a SimBEV dataset already exists (in the path provided by `path`), SimBEV compares the number of existing and desired scenes for each map and creates additional ones if necessary. This feature can be used to continue creating a dataset in the event of a crash or expand an already existing one. To replace undesired scenes, set `mode` to `replace` and denote which scenes should be replaced.

### Post-processing

An optional post-processing step will calculate the number of lidar and radar points inside each 3D object bounding box (0 for all objects if not collected) alongside a flag indicating whether the number of points inside the bounding box is non-zero, and will append this information to bounding box data. To do this, in the [simbev](simbev) directory run
```Bash
python post_processing.py
```
where an optional `path` argument can be used to provided the path to the SimBEV dataset (`path` is `/dataset` by default). This will create a new folder called `new_det` under `ground-truth` (see [Data Format](#data-format) for more information). Delete the `det` folder (or add `.bak` to its name to keep it as backup) and rename `new_det` to `det`.

### Data Visualization

To visualize certain types of collected data (those that are not readily visualized, e.g. semantic segmentation images are already in `.png` format), in the [viz](viz) directory run
```Bash
python visualization.py [mode] [options]
```
which will create a new `viz` folder in the dataset's path where the visualizations are stored. Visualizations involving 3D object bounding boxes require data to be post-processed first.

`mode` can be `all`, or any combination of the following:
* `rgb`: RGB images with 3D object bounding boxes overlaid.
<p align="middle">
  <img src="assets/RGB-CAM_FRONT_LEFT.jpg" width="32%" />
  <img src="assets/RGB-CAM_FRONT.jpg" width="32%" /> 
  <img src="assets/RGB-CAM_FRONT_RIGHT.jpg" width="32%" />
</p>
<p align="middle">
  <img src="assets/RGB-CAM_BACK_RIGHT.jpg" width="32%" />
  <img src="assets/RGB-CAM_BACK.jpg" width="32%" /> 
  <img src="assets/RGB-CAM_BACK_LEFT.jpg" width="32%" />
</p>

* `depth`: depth images.
<p align="middle">
  <img src="assets/DPT-CAM_FRONT_LEFT.jpg" width="32%" />
  <img src="assets/DPT-CAM_FRONT.jpg" width="32%" /> 
  <img src="assets/DPT-CAM_FRONT_RIGHT.jpg" width="32%" />
</p>
<p align="middle">
  <img src="assets/DPT-CAM_BACK_RIGHT.jpg" width="32%" />
  <img src="assets/DPT-CAM_BACK.jpg" width="32%" /> 
  <img src="assets/DPT-CAM_BACK_LEFT.jpg" width="32%" />
</p>

* `flow`: optical flow images.
<p align="middle">
  <img src="assets/FLW-CAM_FRONT_LEFT.jpg" width="32%" />
  <img src="assets/FLW-CAM_FRONT.jpg" width="32%" /> 
  <img src="assets/FLW-CAM_FRONT_RIGHT.jpg" width="32%" />
</p>
<p align="middle">
  <img src="assets/FLW-CAM_BACK_RIGHT.jpg" width="32%" />
  <img src="assets/FLW-CAM_BACK.jpg" width="32%" /> 
  <img src="assets/FLW-CAM_BACK_LEFT.jpg" width="32%" />
</p>

* `lidar`, `lidar-with-bbox`: top-down view of lidar point clouds, without and with 3D object bounding boxes overlaid, respectively.
<p align="middle">
  <img src="assets/LIDAR.jpg" width="48%" />
  <img src="assets/LIDARwBBOX.jpg" width="48%" />
</p>

* `lidar3d`, `lidar3d-with-bbox`: 3D view of lidar point clouds, without and with 3D object bounding boxes overlaid, respectively.
<p align="middle">
  <img src="assets/LIDAR3D.jpg" width="48%" />
  <img src="assets/LIDAR3DwBBOX.jpg" width="48%" />
</p>

* `semantic-lidar`, `semantic-lidar3D`: top-down and 3D view of semantic lidar point clouds, respectively.
<p align="middle">
  <img src="assets/SEG-LIDAR.jpg" width="32%" />
  <img src="assets/SEG-LIDAR3D.jpg" width="64%" />
</p>

* `radar`, `radar-with-bbox`: top-down view of radar point clouds, without and with 3D object bounding boxes overlaid, respectively.
<p align="middle">
  <img src="assets/RADAR.jpg" width="48%" />
  <img src="assets/RADARwBBOX.jpg" width="48%" />
</p>

* `radar3d`, `radar3d-with-bbox`: 3D view of radar point clouds, without and with 3D object bounding boxes overlaid, respectively.
<p align="middle">
  <img src="assets/RADAR3D.jpg" width="48%" />
  <img src="assets/RADAR3DwBBOX.jpg" width="48%" />
</p>

`options` can be any of the following:
* `path`: path to the dataset (`/dataset` by default).
* `s`, `scene`: list of scene numbers to visualize (-1, i.e. all scenes, by default).
* `f`, `frame`: list of frame numbers to visualize (-1, i.e. all frames, by default).

For instance, using
```Bash
python visualization.py rgb depth lidar3d semantic-lidar radar-with-bbox --scene 0 12 27 --frame 3 30 300
```
visualizes RGB images with 3D object bounding boxes overlaid, depth images, lidar point clouds from a 3D perspective view, semantic lidar point clouds from a top-down view, and radar point clouds from a top-down view with 3D object bounding boxes overlaid for frames 3, 30, and 300 of scenes 0, 12, and 27.

### Using the SimBEV Dataset

Consult our implementations of [BEVFusion](https://github.com/GoodarzMehr/bevfusion) and [UniTR](https://github.com/GoodarzMehr/UniTR) for how to use the SimBEV dataset.

## Data Format

### Sensor Setup

<p align="middle">
  <img src="assets/SensorCoord.png" width="58%" />
  <img src="assets/SensorCoordTab.png" width="40%" />
</p>

###### <p align="center"> Placement and coordinate system of different sensors are shown on the left and tabulated on the right. Coordinate values are relative to a FLU (Front-Left-Up) coordinate system positioned at the center of the ground plane of the vehicle's 3D bounding box. </p>

<p align="middle">
  <img src="assets/SensorTab.png" width="98%" />
  <img src="assets/SensorFoV.png" width="98%" />
</p>

###### <p align="center"> Properties of sensors used to collect the SimBEV dataset (top) and their FoV (bottom). </p>

Sensors in SimBEV are referenced using the `{subtype}-{position}` format (which turns into `{position}` when subtype is not available). For cameras, subtype can be one of `RGB` (RGB camera), `SEG` (semantic segmentation camera), `IST` (instance segmentation camera), `DPT` (depth camera), or `FLW` (optical flow camera), while position can be one of `CAM_FRONT_LEFT`, `CAM_FRONT`, `CAM_FRONT_RIGHT`, `CAM_BACK_RIGHT`, `CAM_BACK`, `CAM_BACK_LEFT`. For instance, `DPT-CAM_BACK_LEFT` denotes the back left depth camera. For lidar, since there is only one position, regular lidar is denoted by `LIDAR` while semantic lidar is denoted by `SEG-LIDAR`. For radar, subtype is not available and position can be one of `RAD_LEFT`, `RAD_FRONT`, `RAD_RIGHT`, `RAD_BACK`. GNSS and IMU are simply denoted as `GNSS` and `IMU`, respectively.

### Folder Structure

A SimBEV dataset uses the following folder structure.
```
simbev/
|
├── configs/
|
├── ground-truth/
|   ├── det/
|   ├── seg/
|   ├── seg_viz/
|
├── infos
|   ├── simbev_infos_train.json
|   ├── simbev_infos_val.json
|   ├── simbev_infos_test.json
|
├── logs/
|
├── sweeps/
|   ├── RGB-CAM_FRONT_LEFT/
|   ├── RGB-CAM_FRONT/
|   ├── RGB-CAM_FRONT_RIGHT/
|   ├── RGB-CAM_BACK_LEFT/
|   ├── RGB-CAM_BACK/
|   ├── RGB-CAM_BACK_RIGHT/
|   ├── SEG-CAM_FRONT_LEFT/
|   ├── SEG-CAM_FRONT/
|   ├── SEG-CAM_FRONT_RIGHT/
|   ├── SEG-CAM_BACK_LEFT/
|   ├── SEG-CAM_BACK/
|   ├── SEG-CAM_BACK_RIGHT/
|   ├── IST-CAM_FRONT_LEFT/
|   ├── IST-CAM_FRONT/
|   ├── IST-CAM_FRONT_RIGHT/
|   ├── IST-CAM_BACK_LEFT/
|   ├── IST-CAM_BACK/
|   ├── IST-CAM_BACK_RIGHT/
|   ├── DPT-CAM_FRONT_LEFT/
|   ├── DPT-CAM_FRONT/
|   ├── DPT-CAM_FRONT_RIGHT/
|   ├── DPT-CAM_BACK_LEFT/
|   ├── DPT-CAM_BACK/
|   ├── DPT-CAM_BACK_RIGHT/
|   ├── FLW-CAM_FRONT_LEFT/
|   ├── FLW-CAM_FRONT/
|   ├── FLW-CAM_FRONT_RIGHT/
|   ├── FLW-CAM_BACK_LEFT/
|   ├── FLW-CAM_BACK/
|   ├── FLW-CAM_BACK_RIGHT/
|   ├── LIDAR/
|   ├── SEG-LIDAR/
|   ├── RAD_LEFT/
|   ├── RAD_FRONT/
|   ├── RAD_RIGHT/
|   ├── RAD_BACK/
|   ├── GNSS/
|   ├── IMU/
|
├── viz/ (if data is visualized)
```


#### configs

Contains the config file for each scene, with the files using the `SimBEV-scene-{scene number}.yaml` naming scheme. The files are usually identical, unless the dataset was expanded or some scenes were replaced using a different configuration.

#### ground-truth

Contains the ground truth files for each frame, with the files using the `SimBEV-scene-{scene number}-frame-{frame number}-{type}.{data type}` naming scheme. For the `det`, `seg`, and `seg_viz` folders, `type` and `data type` are `GT_DET` and `bin`; `GT_SEG` and `npz`; and`GT_SEG_VIZ` and `jpg`, respectively.

The `det` folder contains the 3D object ground truth file for each frame. The following information is provided for each object:
* `id`: object ID supplied by CARLA
* `type`: object type, e.g. `vehicle.ford.mustang_2016` or `walker.pedestrian.0051`
* `is_alive`: True if the object is alive, False if destroyed
* `is_active`: True if the object is active, False otherwise
* `is_dormant`: True if the object is dormant, False otherwise
* `parent`: ID of parent object if one exists, `None` otherwise
* `attributes`: object attributes, e.g. `has_lights`, `color`, `role_name`, etc. for a car
* `semantic_tags`: object semantic tags
* `bounding_box`: global coordinates of the corners of the 3D object bounding box
* `linear_velocity`: linear velocity of the object (m/s)
* `angular_velocity`: angular velocity of the object (deg/s)
* `num_lidar_pts`: number of lidar points inside the 3D bounding box (requires post processing)
* `num_radar_pts`: number of radar points inside the 3D bounding box (requires post processing)
* `valid_flag`: True if at least one lidar or radar point falls inside the 3D bounding box, False otherwise (requires post processing)

The `seg` folder contains the BEV ground truth file for each frame. BEV ground truth is a binary $C \times d \times d$ array, where $C$ is the number of classes and $d$ is the dimension of the BEV grid (360 for the SimBEV dataset). Currently, $C$ is 8 and the classes are, in order, `road`, `car`, `truck`, `bus`, `motorcycle`, `bicycle`, `rider`, `pedestrian`. The second and third dimensions of the array increase along the $-x$ and $-y$ axes of the vehicle's FLU coordinate system, respectively.

The `seg_viz` folder contains the visualization of the BEV ground truth for each frame.

#### infos

Contains the info files for each data split, with the files using the `simbev_infos_{split}.json` naming scheme where `split` is either `train`, `val`, or `test`. Each file is comprised of `metadata` and `data`. `metadata` contains coordinate transformation matrices for all sensors (i.e. `sensor2lidar_translation`, `sensor2lidar_rotation`, `sensor2ego_translation`, and `sensor2ego_rotation`), as well as the camera intrinsics matrix. `data` contains scene information, divided into `scene_info` and `scene_data` for each scene. `scene_info` includes overall scene information, while `scene_data` provides information about individual frames, including file paths for collected sensor data and the corresponding ground truth.

![collage](assets/Infos.png)

#### logs

Contains the log file for each scene, with the files using the `SimBEV-scene-{scene number}.log` naming scheme. Log files can be used to replay scenes in CARLA and collect additional data.

#### sweeps

Contains collected sensor data for each frame, with the files using the `{sensor}/SimBEV-scene-{scene number}-frame-{frame number}-{sensor}.{type}` naming scheme. For instance, back left RGB camera image for frame 12 of scene 27 is saved as `RGB-CAM_BACK_LEFT/SimBEV-scene-0027-frame-0012-RGB-CAM_BACK_LEFT.jpg`. We briefly discuss how each sensor's data is saved below. See [CARLA's sensors documentation](https://carla.readthedocs.io/en/latest/ref_sensors/) for more details.
* RGB camera: images are saved as `.jpg` files.
* Semantic segmentation camera: images are saved as `.png` files.
* Instance segmentation camera: images are saved as `.png` files.
* Depth camera: images are saved as `.png` files.
* Optical flow camera: images are saved as a $(h, w, 2)$ NumPy array where $h$ and $w$ are the image height and width, respectively.
* Lidar: point clouds are saved as a $(n, 3)$ NumPy array where the columns represent the $x$, $y$, and $z$ values, respectively.
* Semantic lidar: point clouds are saved as a $(n, 6)$ NumPy array where the columns represent the $x$, $y$, and $z$ values, cosine of the incidence angle, and the index and semantic tag of the hit object, respectively.
* Radar: point clouds are saved as a $(n, 4)$ NumPy array where the columns represent the depth, altitude angle, azimuth angle, and velocity, respectively.
* GNSS: data is saved as a \[latitude, longitude, altitude\] Numpy array.
* IMU: data is saved as a \[ $\dot{x}$, $\dot{y}$, $\dot{z}$, $\dot{\phi}$, $\dot{\theta}$, $\dot{\psi}$, $\psi$\] NumPy array.

## SimBEV Dataset Benchmarks

Models are trained on the SimBEV dataset's _train_ set and evaluated on its _test_ set with the hyperparameters their authors used for the nuScenes dataset.

### 3D Object Detection

| Model       |   Modality | mAP (%) | mATE (m) | mAOE (rad) |  mASE | mAVE (m/s) | SDS (%) |     |
| :---------: | :--------: | :-----: | :------: | :--------: | :---: | :--------: | :-----: | :-: |
| BEVFusion-C |          C |    22.1 |    0.744 |       1.04 | 0.137 |       4.65 |    25.1 | [Checkpoint](https://drive.google.com/file/d/1pV8-peIEbnhRogjyXD4skVt-ZoCo_4yq) |
| BEVFusion-L |          L |    48.1 |    0.144 |      0.133 | 0.134 |       1.56 |    56.4 | [Checkpoint](https://drive.google.com/file/d/1-xvt3JfoSsZqu0BMmg90MnF8sxKWVvx-) |
| BEVFusion   |        C+L |    48.1 |    0.146 |      0.122 | 0.127 |       1.54 |    56.6 | [Checkpoint](https://drive.google.com/file/d/1vZrYQ5tCAlo67jYGQ_4plNYBvctfi5Au) |
| UniTR       |        C+L |    47.7 |    0.113 |      0.224 | 0.090 |       0.55 |    61.7 | [Checkpoint](https://drive.google.com/file/d/1Dsx3JYTNwfgIgkH-XF-tOHNpFhTC_AFv) |
| UniTR+LSS   |        C+L |    47.8 |    0.113 |      0.207 | 0.085 |       0.53 |    62.2 | [Checkpoint](https://drive.google.com/file/d/1-yu5D_VuIGNm5mxnGyf8ubUgP4AQ3TSA) |

### BEV Segmentation

| Model       |   Modality | Road |  Car | Truck |  Bus | Motorcycle | Bicycle | Rider | Pedestrian | mIoU |     |
| :---------: | :--------: | :--: | :--: | :---: | :--: | :--------: | :-----: | :---: | :--------: | :--: | :-: |
| BEVFusion-C |          C | 76.0 | 17.2 |   5.1 | 22.9 |        0.0 |     0.0 |   0.0 |        0.0 | 15.2 | [Checkpoint](https://drive.google.com/file/d/12HJVTAxoVnyZnEBePRaFT44chUi0HPp1) |
| BEVFusion-L |          L | 87.7 | 70.6 |  73.5 | 81.5 |       32.5 |     3.6 |  18.4 |       18.9 | 48.3 | [Checkpoint](https://drive.google.com/file/d/1ga4B6-C45hWVMUTv0CpGjs6kG5cD6aW9) |
| BEVFusion   |        C+L | 88.4 | 72.7 |  74.5 | 80.0 |       36.3 |     3.6 |  23.3 |       20.0 | 50.0 | [Checkpoint](https://drive.google.com/file/d/1ikXihAr3_8KL_Usq426tEkNgeTxpGpkq) |
| UniTR       |        C+L | 92.8 | 73.8 |  67.7 | 51.7 |       36.5 |    11.4 |  36.2 |       27.5 | 49.7 | [Checkpoint](https://drive.google.com/file/d/1BBQOVime8i2lbt9S8-q_KNEoHv_xSMPK) |
| UniTR+LSS   |        C+L | 93.3 | 72.8 |  69.4 | 58.5 |       35.9 |     6.3 |  31.6 |       12.9 | 47.6 | [Checkpoint](https://drive.google.com/file/d/1mTMHVZu-RHYoYDByAanilfFtMneqCf-W) |

## Acknowledgement

SimBEV is based on [CARLA](https://carla.org/) and we are grateful to the team that maintains it. SimBEV has also taken inspiration from the [nuScenes](https://www.nuscenes.org/), [SHIFT](https://www.vis.xyz/shift/), [OPV2V](https://mobility-lab.seas.ucla.edu/opv2v/), and [V2X-Sim](https://ai4ce.github.io/V2X-Sim/index.html) datasets.

## Citation

If SimBEV is useful or relevant to your research, please kindly recognize our contributions by citing our paper:
```bibtex
@article{mehr2025simbev,
  title={SimBEV: A Synthetic Multi-Task Multi-Sensor Driving Data Generation Tool and Dataset},
  author={Mehr, Goodarz and Eskandarian, Azim},
  journal={arXiv preprint arXiv:2502.01894},
  year={2025}
}
```
